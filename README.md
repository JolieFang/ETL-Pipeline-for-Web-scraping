# ETL-Pipeline-for-Web-scraping

The goal of this project is to develop a process that automatically extracts three different data sources - ATP tours, Twitter data and Bet365 data into two different databases - Mongodb and Postgres. Then load it to the containers that was created using Docker. This recorded all the processes within one file under Git, and the ETL pipeline was expected to run once the codes were initiated. The automated process will help stakeholders in efficiently replicating the ETL process, resulting in a reliable and ready-to-scale data processing pipeline.



## Using several tools:

- Docker containers
- PySpark
- PostgreSQL database
- MongoDB database
- Python programming language
- Selenium
- ChromeDriver and Google Chrome
